{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7177418,"sourceType":"datasetVersion","datasetId":4147936},{"sourceId":7388596,"sourceType":"datasetVersion","datasetId":4294849},{"sourceId":7529727,"sourceType":"datasetVersion","datasetId":4330547},{"sourceId":7726238,"sourceType":"datasetVersion","datasetId":4513922},{"sourceId":7726342,"sourceType":"datasetVersion","datasetId":4514001},{"sourceId":7726352,"sourceType":"datasetVersion","datasetId":4514010},{"sourceId":7726469,"sourceType":"datasetVersion","datasetId":4514088}],"dockerImageVersionId":30636,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers==4.36.2\n!pip install --upgrade diffusers[torch]==0.25.1\n!pip install accelerate==0.25.0\n# # !pip install xformers==0.0.23.post1","metadata":{"execution":{"iopub.status.busy":"2024-04-27T11:26:55.511608Z","iopub.execute_input":"2024-04-27T11:26:55.512037Z","iopub.status.idle":"2024-04-27T11:27:33.711897Z","shell.execute_reply.started":"2024-04-27T11:26:55.512005Z","shell.execute_reply":"2024-04-27T11:27:33.710710Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers==4.36.2 in /opt/conda/lib/python3.10/site-packages (4.36.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.36.2) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers==4.36.2) (0.20.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.36.2) (1.24.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.36.2) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.36.2) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.36.2) (2023.8.8)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.36.2) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers==4.36.2) (0.15.0)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.36.2) (0.4.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.36.2) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.36.2) (2023.12.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.36.2) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.36.2) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.36.2) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.36.2) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.36.2) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.36.2) (2023.11.17)\nRequirement already satisfied: diffusers[torch]==0.25.1 in /opt/conda/lib/python3.10/site-packages (0.25.1)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.10/site-packages (from diffusers[torch]==0.25.1) (6.8.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from diffusers[torch]==0.25.1) (3.12.2)\nRequirement already satisfied: huggingface-hub>=0.20.2 in /opt/conda/lib/python3.10/site-packages (from diffusers[torch]==0.25.1) (0.20.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from diffusers[torch]==0.25.1) (1.24.3)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from diffusers[torch]==0.25.1) (2023.8.8)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from diffusers[torch]==0.25.1) (2.31.0)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from diffusers[torch]==0.25.1) (0.4.1)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from diffusers[torch]==0.25.1) (9.5.0)\nRequirement already satisfied: torch>=1.4 in /opt/conda/lib/python3.10/site-packages (from diffusers[torch]==0.25.1) (2.0.0)\nRequirement already satisfied: accelerate>=0.11.0 in /opt/conda/lib/python3.10/site-packages (from diffusers[torch]==0.25.1) (0.25.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.11.0->diffusers[torch]==0.25.1) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.11.0->diffusers[torch]==0.25.1) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.11.0->diffusers[torch]==0.25.1) (6.0.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.2->diffusers[torch]==0.25.1) (2023.12.2)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.2->diffusers[torch]==0.25.1) (4.66.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.2->diffusers[torch]==0.25.1) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.4->diffusers[torch]==0.25.1) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.4->diffusers[torch]==0.25.1) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4->diffusers[torch]==0.25.1) (3.1.2)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata->diffusers[torch]==0.25.1) (3.16.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers[torch]==0.25.1) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers[torch]==0.25.1) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers[torch]==0.25.1) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers[torch]==0.25.1) (2023.11.17)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate>=0.11.0->diffusers[torch]==0.25.1) (3.0.9)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.4->diffusers[torch]==0.25.1) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.4->diffusers[torch]==0.25.1) (1.3.0)\nRequirement already satisfied: accelerate==0.25.0 in /opt/conda/lib/python3.10/site-packages (0.25.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.25.0) (1.24.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.25.0) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.25.0) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate==0.25.0) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.25.0) (2.0.0)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate==0.25.0) (0.20.2)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.25.0) (0.4.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate==0.25.0) (3.0.9)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.25.0) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.25.0) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.25.0) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.25.0) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.25.0) (3.1.2)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate==0.25.0) (2023.12.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate==0.25.0) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate==0.25.0) (4.66.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==0.25.0) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate==0.25.0) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate==0.25.0) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate==0.25.0) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate==0.25.0) (2023.11.17)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==0.25.0) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"offset = 0\nlast_image_index = None#None if don't know the number of images\nQF = 25\nfile_path = '/kaggle/input/image-description/Image_description.txt'","metadata":{"execution":{"iopub.status.busy":"2024-04-27T11:27:33.714810Z","iopub.execute_input":"2024-04-27T11:27:33.715667Z","iopub.status.idle":"2024-04-27T11:27:33.812909Z","shell.execute_reply.started":"2024-04-27T11:27:33.715624Z","shell.execute_reply":"2024-04-27T11:27:33.812065Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"%load_ext autoreload\n%autoreload 2\nfrom diffusers import AutoencoderKL\nfrom diffusers import UNet2DConditionModel\nfrom transformers import CLIPTextModel, CLIPTokenizer\n\nimport os\nimport PIL\nfrom PIL import Image, ImageOps\nimport requests\nimport torch\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport gc\nimport json\nimport time\nimport torch\nimport requests\nfrom tqdm import tqdm\nfrom io import BytesIO\nfrom diffusers import StableDiffusionImg2ImgPipeline\nimport torchvision.transforms as T\nfrom typing import Union, List, Optional, Callable\n\nto_pil = T.ToPILImage()\n\nfrom diffusers import AutoencoderKL\nfrom diffusers import UNet2DConditionModel\n\nfrom transformers import CLIPTextModel, CLIPTokenizer\nimport requests\nfrom io import BytesIO","metadata":{"execution":{"iopub.status.busy":"2024-04-27T11:27:33.814379Z","iopub.execute_input":"2024-04-27T11:27:33.814716Z","iopub.status.idle":"2024-04-27T11:27:33.907349Z","shell.execute_reply.started":"2024-04-27T11:27:33.814690Z","shell.execute_reply":"2024-04-27T11:27:33.906111Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n","output_type":"stream"}]},{"cell_type":"code","source":"def init_seeds(seed):\n  np.random.seed(seed)\n  torch.manual_seed(seed)\n  torch.cuda.manual_seed(seed)\n  torch.cuda.manual_seed_all(seed)\n  os.environ[\"PYTHONHASHSEED\"] = str(seed)\n  os.environ['PYTORCH_CUDA_ALLOC_CONF']='max_split_size_mb:224'\n\ntotensor = T.ToTensor()\ntopil = T.ToPILImage()\n\nhyperparams = {\n    'SEED' : 9222,\n    'STRENGTH' : 0.7,\n    'GUIDANCE_SCALE' : 7.5,\n    'NUM_DENOISING_STEPS' : 4,\n    'NUM_DENOISING_STEPS_FINAL':50,\n    'EPS' : 0.1,\n    'STEP_SIZE' : 0.006,\n    'ITERS' : 1,\n    'CLAMP_MIN' : -1,\n    'CLAMP_MAX' : 1,\n    'GRAD_REPS' : 1,\n    'PROMPT_ATTACK': ''\n}\n\ninit_seeds(hyperparams['SEED'])","metadata":{"execution":{"iopub.status.busy":"2024-04-27T11:27:33.908718Z","iopub.execute_input":"2024-04-27T11:27:33.909018Z","iopub.status.idle":"2024-04-27T11:27:33.993329Z","shell.execute_reply.started":"2024-04-27T11:27:33.908992Z","shell.execute_reply":"2024-04-27T11:27:33.992401Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def recover_image(image, init_image, mask, background=False):\n    image = totensor(image)\n    mask = totensor(mask)\n    init_image = totensor(init_image)\n    if background:\n        result = mask * init_image + (1 - mask) * image\n    else:\n        result = mask * image + (1 - mask) * init_image\n    return topil(result)\n\ndef preprocess(image):\n    w, h = image.size\n    w, h = map(lambda x: x - x % 32, (w, h))  # resize to integer multiple of 32\n    image = image.resize((w, h), resample=Image.LANCZOS)\n    image = np.array(image).astype(np.float32) / 255.0\n    image = image[None].transpose(0, 3, 1, 2)\n    image = torch.from_numpy(image)\n    return 2.0 * image - 1.0\n\ndef prepare_image(image):\n    image = np.array(image.convert(\"RGB\"))\n    image = image[None].transpose(0, 3, 1, 2)\n    image = torch.from_numpy(image).to(dtype=torch.float32) / 127.5 - 1.0\n\n    return image[0]","metadata":{"execution":{"iopub.status.busy":"2024-04-27T11:27:33.996903Z","iopub.execute_input":"2024-04-27T11:27:33.997206Z","iopub.status.idle":"2024-04-27T11:27:34.085226Z","shell.execute_reply.started":"2024-04-27T11:27:33.997170Z","shell.execute_reply":"2024-04-27T11:27:34.084269Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# , KarrasDiffusionSchedulers\n# Load scheduler and models\n\ndevice = 'cuda'\nunet = UNet2DConditionModel.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\",\n    subfolder=\"unet\",\n    torch_dtype=torch.float16,\n).to(device)\n# text_encoder = CLIPTextModel.from_pretrained(\n#     \"openai/clip-vit-large-patch14\",\n#     subfolder=\"unet\", \n# )\ntokenizer = CLIPTokenizer.from_pretrained(\n    \"openai/clip-vit-large-patch14\",\n    torch_dtype = torch.float16    \n    )\n\ntext_encoder = CLIPTextModel.from_pretrained(\n    \"openai/clip-vit-large-patch14\",\n        torch_dtype = torch.float16\n    \n    ).to(device)\n\nvae = AutoencoderKL.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\",\n    subfolder=\"vae\",\n    torch_dtype=torch.float16\n).to(device)\n\n\n# unet.requires_grad_(False)\n# vae.requires_grad_(False)\n\nunet.enable_gradient_checkpointing()\ntext_encoder.gradient_checkpointing_enable()\nvae._set_gradient_checkpointing(module=vae.encoder, value=True)\nvae._set_gradient_checkpointing(module=vae.decoder, value=True)\n# unet.enable_xformers_memory_efficient_attention()\n\n# from diffusers import DiffusionPipel                                                                                                                     ine, DDIMScheduler\nfrom diffusers import DDIMScheduler\nscheduler = DDIMScheduler()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-27T11:27:34.086417Z","iopub.execute_input":"2024-04-27T11:27:34.086800Z","iopub.status.idle":"2024-04-27T11:27:37.185958Z","shell.execute_reply.started":"2024-04-27T11:27:34.086770Z","shell.execute_reply":"2024-04-27T11:27:37.185124Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"preprocess_res = T.Compose([\n    T.Resize(256),\n    T.CenterCrop(224),\n    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\npreprocess_tensor = T.Compose([\n    T.ToTensor(),\n])\n\npreprocess_tensor_pil= T.ToPILImage()","metadata":{"execution":{"iopub.status.busy":"2024-04-27T11:27:37.187189Z","iopub.execute_input":"2024-04-27T11:27:37.187518Z","iopub.status.idle":"2024-04-27T11:27:37.272365Z","shell.execute_reply.started":"2024-04-27T11:27:37.187493Z","shell.execute_reply":"2024-04-27T11:27:37.271279Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"def download_image(url):\n    try:\n        response = requests.get(url)\n        init_image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n        resize = T.transforms.Resize(512)\n        center_crop = T.transforms.CenterCrop(512)\n        init_image = center_crop(resize(init_image))\n\n        return init_image\n    except (requests.exceptions.RequestException, PIL.UnidentifiedImageError) as e:\n    # Handle the exception here\n        print(f\"Error downloading/processing image: {e}\")\n    # You can return None, a default image, or log the error\n        return None  # Example: return None if download fails\n\ndef read_image_prompt(file_path):\n    with open(file_path, 'r') as file:\n        text = file.read()\n    sentences = text.split('\\n')\n    image_url = []\n    prompt = []\n    image_description = []\n    for i, sentence in enumerate(sentences):\n        if i%3==0:\n            image_url.append(sentence)\n        elif i%3==1:\n            prompt.append(sentence)\n        elif i%3==2:\n            image_description.append(sentence)\n            \n\n    return (image_url, prompt, image_description)\n\n\nimage_url, prompt, _ = read_image_prompt(file_path)\nimage_url = image_url[:-1]\n\n#take only some images  #-------------------------------------------------------------->>>>>>>>>>>>updates here\nif last_image_index == None:\n    image_url = image_url[offset:]\n    prompt = prompt[offset:]\n    \nelse:\n    image_url = image_url[offset:last_image_index]\n    prompt = prompt[offset:last_image_index]\n","metadata":{"execution":{"iopub.status.busy":"2024-04-27T11:27:37.273689Z","iopub.execute_input":"2024-04-27T11:27:37.274076Z","iopub.status.idle":"2024-04-27T11:27:37.368679Z","shell.execute_reply.started":"2024-04-27T11:27:37.274041Z","shell.execute_reply":"2024-04-27T11:27:37.367474Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# A differentiable version of the forward function of the inpainting stable diffusion model! See https://github.com/huggingface/diffusers\n# To understand https://huggingface.co/docs/diffusers/using-diffusers/write_own_pipeline\n\n#careful on gradient direction\ndef attack_forward(\n        prompt: Union[str, List[str]],\n        image: Union[torch.FloatTensor, Image.Image],\n        height: int = 512,\n        width: int = 512,\n        num_inference_steps: int = 50,\n        guidance_scale: float = 7.5,\n    ):\n        text_inputs = tokenizer(\n            prompt,\n            padding=\"max_length\",\n            max_length=tokenizer.model_max_length,\n            return_tensors=\"pt\",\n        )\n        text_input_ids = text_inputs.input_ids\n\n        text_embeddings = text_encoder(text_input_ids.to(device))[0]\n\n        uncond_tokens = [\"\"]\n        #unconditional text embeddings for the padding token\n        max_length = text_input_ids.shape[-1]\n        uncond_input = tokenizer(\n            uncond_tokens,\n            padding=\"max_length\",\n            max_length=max_length,\n            truncation=True,\n            return_tensors=\"pt\",\n        )\n        uncond_embeddings = text_encoder(uncond_input.input_ids.to(device))[0]\n        seq_len = uncond_embeddings.shape[1]\n        text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n        text_embeddings = text_embeddings.detach()\n\n        num_channels_latents = vae.config.latent_channels\n\n        # latents_shape = (1 , num_channels_latents, height // 8, width // 8)\n        # latents = torch.randn(latents_shape, device=self.device, dtype=text_embeddings.dtype)\n        image_latents = vae.encode(image).latent_dist.sample()\n        image_latents = 0.18215 * image_latents\n        # image_latents = torch.cat([image_latents] * 2)\n        \n        latents = image_latents * scheduler.init_noise_sigma\n        scheduler.set_timesteps(num_inference_steps)\n        timesteps_tensor = scheduler.timesteps.to(device)\n\n        for i, t in enumerate(timesteps_tensor):\n            # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n            latent_model_input = torch.cat([latents] * 2)\n            latent_model_input = scheduler.scale_model_input(latent_model_input, timestep=t)\n            # latent_model_input = torch.cat([latent_model_input, image_latents], dim=1)\n            noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n            \n            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n            #use the noise prediction of both image and text\n            noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n            # compute the previous noisy sample x_t -> x_t-1\n            latents = scheduler.step(noise_pred, t, latents).prev_sample #why\n            \n            del latent_model_input\n            del noise_pred_uncond, noise_pred_text\n            del noise_pred\n            torch.cuda.empty_cache()\n            \n        latents = 1 / 0.18215 * latents #get the latents\n        image = vae.decode(latents).sample\n        return image\n\n\ndef compute_grad(X, cur_image, prompt,  **kwargs):\n    torch.set_grad_enabled(True)\n    cur_image = cur_image.clone()\n    cur_image.requires_grad_()\n    image = attack_forward(\n                               image=cur_image,\n                               prompt=prompt,\n                               **kwargs)\n#     loss = (image_nat - target_image).norm(p=2)\n#     loss = lpips(image.clamp(-1,1), X)\n    loss = image.norm(p=2)\n    grad = torch.autograd.grad(loss, [cur_image])[0]\n    \n    return grad, loss.item(), image.data.cpu()\n\ndef super_l2( X, prompt, step_size, iters, eps, clamp_min, clamp_max, grad_reps = 5, **kwargs):\n    X_adv = X.clone()\n    iterator = tqdm(range(iters))\n\n    for i in iterator:\n\n        all_grads = []\n        losses = []\n\n        for i in range(grad_reps): #Why do this for same image, prompt and parameters?\n            c_grad, loss, last_image = compute_grad(X, X_adv, prompt, **kwargs)\n            all_grads.append(c_grad)\n            losses.append(loss)\n        \n        grad = torch.stack(all_grads).mean(0)\n\n        iterator.set_description_str(f'AVG Loss: {np.mean(losses):.3f}')\n\n        l = len(X.shape) - 1   #why?\n        grad_norm = torch.norm(grad.detach().reshape(grad.shape[0], -1), dim=1).view(-1, *([1] * l))\n        grad_normalized = grad.detach() / (grad_norm + 1e-10)\n\n        # actual_step_size = step_size - (step_size - step_size / 100) / iters * i\n        actual_step_size = step_size\n        X_adv = X_adv - grad_normalized * actual_step_size\n\n        d_x = X_adv - X.detach()  #Perturbations\n        d_x_norm = torch.renorm(d_x, p=2, dim=0, maxnorm=eps)   #Why?\n        X_adv.data = torch.clamp(X + d_x_norm, clamp_min, clamp_max)\n    \n\n        torch.cuda.empty_cache()\n\n    return X_adv, last_image\n\ndef super_linf(X,jpeg_model, prompt, step_size, iters, eps, clamp_min, clamp_max, grad_reps = 5, **kwargs):\n    X_adv = X.clone()\n    iterator = tqdm(range(iters))\n\n    for i in iterator:\n        all_grads_jpeg = []\n        losses = []\n        \n        QFs = [25, 50, 75, 100]\n        \n        for QF in QFs:\n            X_jpeg = jpeg_model(X_adv.float(), QF).half()\n            all_grads = []\n\n            for i in range(grad_reps):\n                c_grad, loss, last_image = compute_grad(X, X_jpeg, prompt, **kwargs)\n                all_grads.append(c_grad)\n                losses.append(loss)\n            grad = torch.stack(all_grads).mean(0)\n            iterator.set_description_str(f'AVG Loss: {np.mean(losses):.3f}')\n            all_grads_jpeg.append(grad)\n            \n        grad = torch.stack(all_grads_jpeg).mean(0)\n        \n        actual_step_size = step_size - (step_size - step_size / 100) / iters * i\n#         actual_step_size = step_size\n\n        X_adv = X_adv - grad.detach().sign() * actual_step_size\n\n        X_adv = torch.minimum(torch.maximum(X_adv, X - eps), X + eps)\n        X_adv.data = torch.clamp(X_adv, min=clamp_min, max=clamp_max)\n\n\n    torch.cuda.empty_cache()\n\n    return X_adv, last_image, np.mean(losses)","metadata":{"execution":{"iopub.status.busy":"2024-04-27T11:27:37.370345Z","iopub.execute_input":"2024-04-27T11:27:37.370647Z","iopub.status.idle":"2024-04-27T11:27:37.476973Z","shell.execute_reply.started":"2024-04-27T11:27:37.370622Z","shell.execute_reply":"2024-04-27T11:27:37.476025Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"def gen_image(init_image, adv_image, path_to_save, prompt,  QF:int = 60):\n\n    (height, width) = (512, 512)\n    with torch.no_grad():\n\n        adv_image.save(f'{path_to_save}/adv_image.png')\n        init_image.save(f'{path_to_save}/original_image.png')\n        num_compressions = (95-QF)//10\n        jump = 10\n\n        fig, ax = plt.subplots(nrows=2, ncols=num_compressions+2, figsize=(20,6))\n        ax[0,0].imshow(init_image)\n        ax[0,1].imshow(adv_image)\n        # Play with these for improving generated image quality\n        # SEED = np.random.randint(low=0, high=10000)\n\n        STRENGTH = hyperparams['STRENGTH']\n        GUIDANCE = hyperparams['GUIDANCE_SCALE']\n        NUM_STEPS = hyperparams['NUM_DENOISING_STEPS_FINAL']\n        SEED  = hyperparams['SEED']\n        with torch.autocast('cuda'):\n            torch.manual_seed(SEED)\n            image_nat = pipe_img2img(prompt=prompt, image=init_image, strength=STRENGTH, guidance_scale=GUIDANCE, num_inference_steps=NUM_STEPS).images[0]\n            torch.manual_seed(SEED)\n            image_adv = pipe_img2img(prompt=prompt, image=adv_image, strength=STRENGTH, guidance_scale=GUIDANCE, num_inference_steps=NUM_STEPS).images[0]\n\n        image_nat.save(f'{path_to_save}/gen_original_image.png')\n        image_adv.save(f'{path_to_save}/gen_adversarial.png')\n        ax[1,0].imshow(image_nat)\n        ax[1,1].imshow(image_adv)\n        ax[0,0].set_title('Source', fontsize=16)\n        ax[0,1].set_title('Adv.', fontsize=16)\n\n        for i in range(num_compressions):\n\n            jpeg = DiffJPEG(height=height, width=width, differentiable=True).to('cuda')\n            compressed_img = jpeg(preprocess_tensor(adv_image).unsqueeze(0).to('cuda') , quality=QF)\n\n            preprocess_tensor_pil(compressed_img[0]).save(f\"{path_to_save}/adv_image_compress_{QF}.png\")\n            jpg_image = Image.open(f\"{path_to_save}/adv_image_compress_{QF}.png\")\n            with torch.autocast('cuda'):\n                torch.manual_seed(SEED)\n                image_adv_jpeg = pipe_img2img(prompt=prompt, image=jpg_image, strength=STRENGTH, guidance_scale=GUIDANCE, num_inference_steps=NUM_STEPS).images[0]\n                image_adv_jpeg.save(f'{path_to_save}/gen_adv_image{QF}.png')\n\n                ax[0,i+2].imshow(jpg_image)\n                ax[1,i+2].imshow(image_adv_jpeg)\n                ax[0,i+2].set_title(f'QF: {QF}.', fontsize=16)\n            QF = QF + jump\n\n\n        for i in range(2):\n            for j in range(num_compressions):\n                ax[i][j].grid(False)\n                ax[i][j].axis('off')\n\n        fig.suptitle(f\"Prompt: {prompt}\", fontsize=20)\n        fig.tight_layout()\n#         plt.show(plt.savefig(f\"{path_to_save}/Result.png\", bbox_inches='tight'))\n        plt.savefig(f\"{path_to_save}/Result.png\", bbox_inches='tight')\n\n        \ndef save_dict_to_text_file(dictionary, file_path, total_time, loss):\n    \"\"\"\n    Convert a dictionary to a JSON-formatted string and save it to a text file.\n\n    Parameters:\n    - dictionary: The dictionary to be saved.\n    - file_path: The target location and filename for the text file.\n    \"\"\"\n    try:\n        with open(f'{file_path}/config.txt', 'w') as file:\n            json.dump(dictionary, file, indent=4)\n            file.write(f'total time: {total_time}')\n            for i, val in enumerate(loss):\n                file.write(f'Loss Sample {i}: {val}')\n\n        print(f\"Dictionary successfully saved to {file_path}\")\n    except Exception as e:\n        print(f\"Error saving dictionary to {file_path}: {e}\")\n    \n    \n","metadata":{"execution":{"iopub.status.busy":"2024-04-27T11:27:37.478532Z","iopub.execute_input":"2024-04-27T11:27:37.479166Z","iopub.status.idle":"2024-04-27T11:27:37.575170Z","shell.execute_reply.started":"2024-04-27T11:27:37.479128Z","shell.execute_reply":"2024-04-27T11:27:37.574277Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# We didn't write this ourselves but I don't remember from where we took this will find it's source and update\n\n\n# Standard libraries\nimport itertools\nimport numpy as np\n# PyTorch\nimport torch\nimport torch.nn as nn\n\ny_table = np.array(\n    [[16, 11, 10, 16, 24, 40, 51, 61], [12, 12, 14, 19, 26, 58, 60,\n                                        55], [14, 13, 16, 24, 40, 57, 69, 56],\n     [14, 17, 22, 29, 51, 87, 80, 62], [18, 22, 37, 56, 68, 109, 103,\n                                        77], [24, 35, 55, 64, 81, 104, 113, 92],\n     [49, 64, 78, 87, 103, 121, 120, 101], [72, 92, 95, 98, 112, 100, 103, 99]],\n    dtype=np.float32).T\n\ny_table = nn.Parameter(torch.from_numpy(y_table))\n#\nc_table = np.empty((8, 8), dtype=np.float32)\nc_table.fill(99)\nc_table[:4, :4] = np.array([[17, 18, 24, 47], [18, 21, 26, 66],\n                            [24, 26, 56, 99], [47, 66, 99, 99]]).T\nc_table = nn.Parameter(torch.from_numpy(c_table))\n\n\ndef diff_round(x):\n    \"\"\" Differentiable rounding function\n    Input:\n        x(tensor)\n    Output:\n        x(tensor)\n    \"\"\"\n    return torch.round(x) + (x - torch.round(x))**3\n\n\ndef quality_to_factor(quality):\n    \"\"\" Calculate factor corresponding to quality\n    Input:\n        quality(float): Quality for jpeg compression\n    Output:\n        factor(float): Compression factor\n    \"\"\"\n    if quality < 50:\n        quality = 5000. / quality\n    else:\n        quality = 200. - quality*2\n    return quality / 100.\n\n\nclass y_dequantize(nn.Module):\n    \"\"\" Dequantize Y channel\n    Inputs:\n        image(tensor): batch x height x width\n        factor(float): compression factor\n    Outputs:\n        image(tensor): batch x height x width\n\n    \"\"\"\n    def __init__(self):\n        super(y_dequantize, self).__init__()\n        self.y_table = y_table\n\n    def forward(self, image, factor):\n        return image * (self.y_table * factor)\n\n\nclass c_dequantize(nn.Module):\n    \"\"\" Dequantize CbCr channel\n    Inputs:\n        image(tensor): batch x height x width\n        factor(float): compression factor\n    Outputs:\n        image(tensor): batch x height x width\n\n    \"\"\"\n    def __init__(self):\n        super(c_dequantize, self).__init__()\n        self.c_table = c_table\n\n    def forward(self, image , factor):\n        return image * (self.c_table * factor)\n\n\nclass idct_8x8(nn.Module):\n    \"\"\" Inverse discrete Cosine Transformation\n    Input:\n        dcp(tensor): batch x height x width\n    Output:\n        image(tensor): batch x height x width\n    \"\"\"\n    def __init__(self):\n        super(idct_8x8, self).__init__()\n        alpha = np.array([1. / np.sqrt(2)] + [1] * 7)\n        self.alpha = nn.Parameter(torch.from_numpy(np.outer(alpha, alpha)).float())\n        tensor = np.zeros((8, 8, 8, 8), dtype=np.float32)\n        for x, y, u, v in itertools.product(range(8), repeat=4):\n            tensor[x, y, u, v] = np.cos((2 * u + 1) * x * np.pi / 16) * np.cos(\n                (2 * v + 1) * y * np.pi / 16)\n        self.tensor = nn.Parameter(torch.from_numpy(tensor).float())\n\n    def forward(self, image):\n\n        image = image * self.alpha\n        result = 0.25 * torch.tensordot(image, self.tensor, dims=2) + 128\n        result.view(image.shape)\n        return result\n\n\nclass block_merging(nn.Module):\n    \"\"\" Merge pathces into image\n    Inputs:\n        patches(tensor) batch x height*width/64, height x width\n        height(int)\n        width(int)\n    Output:\n        image(tensor): batch x height x width\n    \"\"\"\n    def __init__(self):\n        super(block_merging, self).__init__()\n\n    def forward(self, patches, height, width):\n        k = 8\n        batch_size = patches.shape[0]\n        image_reshaped = patches.view(batch_size, height//k, width//k, k, k)\n        image_transposed = image_reshaped.permute(0, 1, 3, 2, 4)\n        return image_transposed.contiguous().view(batch_size, height, width)\n\n\nclass chroma_upsampling(nn.Module):\n    \"\"\" Upsample chroma layers\n    Input:\n        y(tensor): y channel image\n        cb(tensor): cb channel\n        cr(tensor): cr channel\n    Ouput:\n        image(tensor): batch x height x width x 3\n    \"\"\"\n    def __init__(self):\n        super(chroma_upsampling, self).__init__()\n\n    def forward(self, y, cb, cr):\n        def repeat(x, k=2):\n            height, width = x.shape[1:3]\n            x = x.unsqueeze(-1)\n            x = x.repeat(1, 1, k, k)\n            x = x.view(-1, height * k, width * k)\n            return x\n\n        cb = repeat(cb)\n        cr = repeat(cr)\n\n        return torch.cat([y.unsqueeze(3), cb.unsqueeze(3), cr.unsqueeze(3)], dim=3)\n\n\nclass ycbcr_to_rgb_jpeg(nn.Module):\n    \"\"\" Converts YCbCr image to RGB JPEG\n    Input:\n        image(tensor): batch x height x width x 3\n    Outpput:\n        result(tensor): batch x 3 x height x width\n    \"\"\"\n    def __init__(self):\n        super(ycbcr_to_rgb_jpeg, self).__init__()\n\n        matrix = np.array(\n            [[1., 0., 1.402], [1, -0.344136, -0.714136], [1, 1.772, 0]],\n            dtype=np.float32).T\n        self.shift = nn.Parameter(torch.tensor([0, -128., -128.]))\n        self.matrix = nn.Parameter(torch.from_numpy(matrix))\n\n    def forward(self, image):\n        result = torch.tensordot(image + self.shift, self.matrix, dims=1)\n        #result = torch.from_numpy(result)\n        result.view(image.shape)\n        return result.permute(0, 3, 1, 2)\n\n\nclass decompress_jpeg(nn.Module):\n    \"\"\" Full JPEG decompression algortihm\n    Input:\n        compressed(dict(tensor)): batch x h*w/64 x 8 x 8\n        rounding(function): rounding function to use\n        factor(float): Compression factor\n    Ouput:\n        image(tensor): batch x 3 x height x width\n    \"\"\"\n    def __init__(self, height, width, rounding=torch.round):\n        super(decompress_jpeg, self).__init__()\n        self.c_dequantize = c_dequantize()\n        self.y_dequantize = y_dequantize()\n        self.idct = idct_8x8()\n        self.merging = block_merging()\n        self.chroma = chroma_upsampling()\n        self.colors = ycbcr_to_rgb_jpeg()\n\n        self.height, self.width = height, width\n\n    def forward(self, y, cb, cr , factor):\n        components = {'y': y, 'cb': cb, 'cr': cr}\n        for k in components.keys():\n            if k in ('cb', 'cr'):\n                comp = self.c_dequantize(components[k], factor)\n                height, width = int(self.height/2), int(self.width/2)\n            else:\n                comp = self.y_dequantize(components[k], factor)\n                height, width = self.height, self.width\n            comp = self.idct(comp)\n            components[k] = self.merging(comp, height, width)\n            #\n        image = self.chroma(components['y'], components['cb'], components['cr'])\n        image = self.colors(image)\n\n        image = torch.min(255*torch.ones_like(image),\n                          torch.max(torch.zeros_like(image), image))\n        return image/255\n\n\n# Standard libraries\nimport itertools\nimport numpy as np\n# PyTorch\nimport torch\nimport torch.nn as nn\n\n\nclass rgb_to_ycbcr_jpeg(nn.Module):\n    \"\"\" Converts RGB image to YCbCr\n    Input:\n        image(tensor): batch x 3 x height x width\n    Outpput:\n        result(tensor): batch x height x width x 3\n    \"\"\"\n    def __init__(self):\n        super(rgb_to_ycbcr_jpeg, self).__init__()\n        matrix = np.array(\n            [[0.299, 0.587, 0.114], [-0.168736, -0.331264, 0.5],\n             [0.5, -0.418688, -0.081312]], dtype=np.float32).T\n        self.shift = nn.Parameter(torch.tensor([0., 128., 128.]))\n        #\n        self.matrix = nn.Parameter(torch.from_numpy(matrix))\n\n    def forward(self, image):\n        image = image.permute(0, 2, 3, 1)\n        result = torch.tensordot(image, self.matrix, dims=1) + self.shift\n    #    result = torch.from_numpy(result)\n        result.view(image.shape)\n        return result\n\n\n\nclass chroma_subsampling(nn.Module):\n    \"\"\" Chroma subsampling on CbCv channels\n    Input:\n        image(tensor): batch x height x width x 3\n    Output:\n        y(tensor): batch x height x width\n        cb(tensor): batch x height/2 x width/2\n        cr(tensor): batch x height/2 x width/2\n    \"\"\"\n    def __init__(self):\n        super(chroma_subsampling, self).__init__()\n\n    def forward(self, image):\n        image_2 = image.permute(0, 3, 1, 2).clone()\n        avg_pool = nn.AvgPool2d(kernel_size=2, stride=(2, 2),\n                                count_include_pad=False)\n        cb = avg_pool(image_2[:, 1, :, :].unsqueeze(1))\n        cr = avg_pool(image_2[:, 2, :, :].unsqueeze(1))\n        cb = cb.permute(0, 2, 3, 1)\n        cr = cr.permute(0, 2, 3, 1)\n        return image[:, :, :, 0], cb.squeeze(3), cr.squeeze(3)\n\n\nclass block_splitting(nn.Module):\n    \"\"\" Splitting image into patches\n    Input:\n        image(tensor): batch x height x width\n    Output:\n        patch(tensor):  batch x h*w/64 x h x w\n    \"\"\"\n    def __init__(self):\n        super(block_splitting, self).__init__()\n        self.k = 8\n\n    def forward(self, image):\n        height, width = image.shape[1:3]\n        batch_size = image.shape[0]\n        image_reshaped = image.view(batch_size, height // self.k, self.k, -1, self.k)\n        image_transposed = image_reshaped.permute(0, 1, 3, 2, 4)\n        return image_transposed.contiguous().view(batch_size, -1, self.k, self.k)\n\n\nclass dct_8x8(nn.Module):\n    \"\"\" Discrete Cosine Transformation\n    Input:\n        image(tensor): batch x height x width\n    Output:\n        dcp(tensor): batch x height x width\n    \"\"\"\n    def __init__(self):\n        super(dct_8x8, self).__init__()\n        tensor = np.zeros((8, 8, 8, 8), dtype=np.float32)\n        for x, y, u, v in itertools.product(range(8), repeat=4):\n            tensor[x, y, u, v] = np.cos((2 * x + 1) * u * np.pi / 16) * np.cos(\n                (2 * y + 1) * v * np.pi / 16)\n        alpha = np.array([1. / np.sqrt(2)] + [1] * 7)\n        #\n        self.tensor =  nn.Parameter(torch.from_numpy(tensor).float())\n        self.scale = nn.Parameter(torch.from_numpy(np.outer(alpha, alpha) * 0.25).float() )\n\n    def forward(self, image):\n        image = image - 128\n        result = self.scale * torch.tensordot(image, self.tensor, dims=2)\n        result.view(image.shape)\n        return result\n\n\nclass y_quantize(nn.Module):\n    \"\"\" JPEG Quantization for Y channel\n    Input:\n        image(tensor): batch x height x width\n        rounding(function): rounding function to use\n        factor(float): Degree of compression\n    Output:\n        image(tensor): batch x height x width\n    \"\"\"\n    def __init__(self, rounding):\n        super(y_quantize, self).__init__()\n        self.rounding = rounding\n        self.y_table = y_table\n\n    def forward(self, image, factor):\n        image = image.float() / (self.y_table * factor)\n        image = self.rounding(image)\n        return image\n\n\nclass c_quantize(nn.Module):\n    \"\"\" JPEG Quantization for CrCb channels\n    Input:\n        image(tensor): batch x height x width\n        rounding(function): rounding function to use\n        factor(float): Degree of compression\n    Output:\n        image(tensor): batch x height x width\n    \"\"\"\n    def __init__(self, rounding):\n        super(c_quantize, self).__init__()\n        self.rounding = rounding\n        self.c_table = c_table\n\n    def forward(self, image, factor):\n        image = image.float() / (self.c_table * factor)\n        image = self.rounding(image)\n        return image\n\n\nclass compress_jpeg(nn.Module):\n    \"\"\" Full JPEG compression algortihm\n    Input:\n        imgs(tensor): batch x 3 x height x width\n        rounding(function): rounding function to use\n        factor(float): Compression factor\n    Ouput:\n        compressed(dict(tensor)): batch x h*w/64 x 8 x 8\n    \"\"\"\n    def __init__(self, rounding=torch.round):\n        super(compress_jpeg, self).__init__()\n        self.l1 = nn.Sequential(\n            rgb_to_ycbcr_jpeg(),\n            chroma_subsampling()\n        )\n        self.l2 = nn.Sequential(\n            block_splitting(),\n            dct_8x8()\n        )\n        self.c_quantize = c_quantize(rounding=rounding)\n        self.y_quantize = y_quantize(rounding=rounding)\n\n    def forward(self, image, factor=1):\n        y, cb, cr = self.l1(image*255)\n        components = {'y': y, 'cb': cb, 'cr': cr}\n        for k in components.keys():\n            comp = self.l2(components[k])\n            if k in ('cb', 'cr'):\n                comp = self.c_quantize(comp, factor)\n            else:\n                comp = self.y_quantize(comp, factor)\n\n            components[k] = comp\n\n        return components['y'], components['cb'], components['cr']\n\nclass DiffJPEG(nn.Module):\n    def __init__(self, height, width, differentiable=True):\n        ''' Initialize the DiffJPEG layer\n        Inputs:\n            height(int): Original image hieght\n            width(int): Original image width\n            differentiable(bool): If true uses custom differentiable\n                rounding function, if false uses standrard torch.round\n            quality(float): Quality factor for jpeg compression scheme.\n        '''\n        super(DiffJPEG, self).__init__()\n        if differentiable:\n            rounding = diff_round\n        else:\n            rounding = torch.round\n        self.compress = compress_jpeg(rounding=rounding)\n        self.decompress = decompress_jpeg(height, width, rounding=rounding)\n\n    def forward(self, x, quality=80):\n        '''\n\n        '''\n\n        factor = quality_to_factor(quality)\n        if factor == 0:\n          return x\n\n        y, cb, cr = self.compress(x, factor)\n        recovered = self.decompress(y, cb, cr, factor)\n        return recovered\n\n\n    \n# Standard libraries\nimport itertools\nimport numpy as np\n# PyTorch\nimport torch\nimport torch.nn as nn\n\ny_table = np.array(\n    [[16, 11, 10, 16, 24, 40, 51, 61], [12, 12, 14, 19, 26, 58, 60,\n                                        55], [14, 13, 16, 24, 40, 57, 69, 56],\n     [14, 17, 22, 29, 51, 87, 80, 62], [18, 22, 37, 56, 68, 109, 103,\n                                        77], [24, 35, 55, 64, 81, 104, 113, 92],\n     [49, 64, 78, 87, 103, 121, 120, 101], [72, 92, 95, 98, 112, 100, 103, 99]],\n    dtype=np.float32).T\n\ny_table = nn.Parameter(torch.from_numpy(y_table))\n#\nc_table = np.empty((8, 8), dtype=np.float32)\nc_table.fill(99)\nc_table[:4, :4] = np.array([[17, 18, 24, 47], [18, 21, 26, 66],\n                            [24, 26, 56, 99], [47, 66, 99, 99]]).T\nc_table = nn.Parameter(torch.from_numpy(c_table))\n\n\ndef diff_round(x):\n    \"\"\" Differentiable rounding function\n    Input:\n        x(tensor)\n    Output:\n        x(tensor)\n    \"\"\"\n    return torch.round(x) + (x - torch.round(x))**3\n\n\ndef quality_to_factor(quality):\n    \"\"\" Calculate factor corresponding to quality\n    Input:\n        quality(float): Quality for jpeg compression\n    Output:\n        factor(float): Compression factor\n    \"\"\"\n    if quality < 50:\n        quality = 5000. / quality\n    else:\n        quality = 200. - quality*2\n    return quality / 100.\n\n\nclass y_dequantize(nn.Module):\n    \"\"\" Dequantize Y channel\n    Inputs:\n        image(tensor): batch x height x width\n        factor(float): compression factor\n    Outputs:\n        image(tensor): batch x height x width\n\n    \"\"\"\n    def __init__(self):\n        super(y_dequantize, self).__init__()\n        self.y_table = y_table\n\n    def forward(self, image, factor):\n        return image * (self.y_table * factor)\n\n\nclass c_dequantize(nn.Module):\n    \"\"\" Dequantize CbCr channel\n    Inputs:\n        image(tensor): batch x height x width\n        factor(float): compression factor\n    Outputs:\n        image(tensor): batch x height x width\n\n    \"\"\"\n    def __init__(self):\n        super(c_dequantize, self).__init__()\n        self.c_table = c_table\n\n    def forward(self, image , factor):\n        return image * (self.c_table * factor)\n\n\nclass idct_8x8(nn.Module):\n    \"\"\" Inverse discrete Cosine Transformation\n    Input:\n        dcp(tensor): batch x height x width\n    Output:\n        image(tensor): batch x height x width\n    \"\"\"\n    def __init__(self):\n        super(idct_8x8, self).__init__()\n        alpha = np.array([1. / np.sqrt(2)] + [1] * 7)\n        self.alpha = nn.Parameter(torch.from_numpy(np.outer(alpha, alpha)).float())\n        tensor = np.zeros((8, 8, 8, 8), dtype=np.float32)\n        for x, y, u, v in itertools.product(range(8), repeat=4):\n            tensor[x, y, u, v] = np.cos((2 * u + 1) * x * np.pi / 16) * np.cos(\n                (2 * v + 1) * y * np.pi / 16)\n        self.tensor = nn.Parameter(torch.from_numpy(tensor).float())\n\n    def forward(self, image):\n\n        image = image * self.alpha\n        result = 0.25 * torch.tensordot(image, self.tensor, dims=2) + 128\n        result.view(image.shape)\n        return result\n\n\nclass block_merging(nn.Module):\n    \"\"\" Merge pathces into image\n    Inputs:\n        patches(tensor) batch x height*width/64, height x width\n        height(int)\n        width(int)\n    Output:\n        image(tensor): batch x height x width\n    \"\"\"\n    def __init__(self):\n        super(block_merging, self).__init__()\n\n    def forward(self, patches, height, width):\n        k = 8\n        batch_size = patches.shape[0]\n        image_reshaped = patches.view(batch_size, height//k, width//k, k, k)\n        image_transposed = image_reshaped.permute(0, 1, 3, 2, 4)\n        return image_transposed.contiguous().view(batch_size, height, width)\n\n\nclass chroma_upsampling(nn.Module):\n    \"\"\" Upsample chroma layers\n    Input:\n        y(tensor): y channel image\n        cb(tensor): cb channel\n        cr(tensor): cr channel\n    Ouput:\n        image(tensor): batch x height x width x 3\n    \"\"\"\n    def __init__(self):\n        super(chroma_upsampling, self).__init__()\n\n    def forward(self, y, cb, cr):\n        def repeat(x, k=2):\n            height, width = x.shape[1:3]\n            x = x.unsqueeze(-1)\n            x = x.repeat(1, 1, k, k)\n            x = x.view(-1, height * k, width * k)\n            return x\n\n        cb = repeat(cb)\n        cr = repeat(cr)\n\n        return torch.cat([y.unsqueeze(3), cb.unsqueeze(3), cr.unsqueeze(3)], dim=3)\n\n\nclass ycbcr_to_rgb_jpeg(nn.Module):\n    \"\"\" Converts YCbCr image to RGB JPEG\n    Input:\n        image(tensor): batch x height x width x 3\n    Outpput:\n        result(tensor): batch x 3 x height x width\n    \"\"\"\n    def __init__(self):\n        super(ycbcr_to_rgb_jpeg, self).__init__()\n\n        matrix = np.array(\n            [[1., 0., 1.402], [1, -0.344136, -0.714136], [1, 1.772, 0]],\n            dtype=np.float32).T\n        self.shift = nn.Parameter(torch.tensor([0, -128., -128.]))\n        self.matrix = nn.Parameter(torch.from_numpy(matrix))\n\n    def forward(self, image):\n        result = torch.tensordot(image + self.shift, self.matrix, dims=1)\n        #result = torch.from_numpy(result)\n        result.view(image.shape)\n        return result.permute(0, 3, 1, 2)\n\n\nclass decompress_jpeg(nn.Module):\n    \"\"\" Full JPEG decompression algortihm\n    Input:\n        compressed(dict(tensor)): batch x h*w/64 x 8 x 8\n        rounding(function): rounding function to use\n        factor(float): Compression factor\n    Ouput:\n        image(tensor): batch x 3 x height x width\n    \"\"\"\n    def __init__(self, height, width, rounding=torch.round):\n        super(decompress_jpeg, self).__init__()\n        self.c_dequantize = c_dequantize()\n        self.y_dequantize = y_dequantize()\n        self.idct = idct_8x8()\n        self.merging = block_merging()\n        self.chroma = chroma_upsampling()\n        self.colors = ycbcr_to_rgb_jpeg()\n\n        self.height, self.width = height, width\n\n    def forward(self, y, cb, cr , factor):\n        components = {'y': y, 'cb': cb, 'cr': cr}\n        for k in components.keys():\n            if k in ('cb', 'cr'):\n                comp = self.c_dequantize(components[k], factor)\n                height, width = int(self.height/2), int(self.width/2)\n            else:\n                comp = self.y_dequantize(components[k], factor)\n                height, width = self.height, self.width\n            comp = self.idct(comp)\n            components[k] = self.merging(comp, height, width)\n            #\n        image = self.chroma(components['y'], components['cb'], components['cr'])\n        image = self.colors(image)\n\n        image = torch.min(255*torch.ones_like(image),\n                          torch.max(torch.zeros_like(image), image))\n        return image/255\n\n\n# Standard libraries\nimport itertools\nimport numpy as np\n# PyTorch\nimport torch\nimport torch.nn as nn\n\n\nclass rgb_to_ycbcr_jpeg(nn.Module):\n    \"\"\" Converts RGB image to YCbCr\n    Input:\n        image(tensor): batch x 3 x height x width\n    Outpput:\n        result(tensor): batch x height x width x 3\n    \"\"\"\n    def __init__(self):\n        super(rgb_to_ycbcr_jpeg, self).__init__()\n        matrix = np.array(\n            [[0.299, 0.587, 0.114], [-0.168736, -0.331264, 0.5],\n             [0.5, -0.418688, -0.081312]], dtype=np.float32).T\n        self.shift = nn.Parameter(torch.tensor([0., 128., 128.]))\n        #\n        self.matrix = nn.Parameter(torch.from_numpy(matrix))\n\n    def forward(self, image):\n        image = image.permute(0, 2, 3, 1)\n        result = torch.tensordot(image, self.matrix, dims=1) + self.shift\n    #    result = torch.from_numpy(result)\n        result.view(image.shape)\n        return result\n\n\n\nclass chroma_subsampling(nn.Module):\n    \"\"\" Chroma subsampling on CbCv channels\n    Input:\n        image(tensor): batch x height x width x 3\n    Output:\n        y(tensor): batch x height x width\n        cb(tensor): batch x height/2 x width/2\n        cr(tensor): batch x height/2 x width/2\n    \"\"\"\n    def __init__(self):\n        super(chroma_subsampling, self).__init__()\n\n    def forward(self, image):\n        image_2 = image.permute(0, 3, 1, 2).clone()\n        avg_pool = nn.AvgPool2d(kernel_size=2, stride=(2, 2),\n                                count_include_pad=False)\n        cb = avg_pool(image_2[:, 1, :, :].unsqueeze(1))\n        cr = avg_pool(image_2[:, 2, :, :].unsqueeze(1))\n        cb = cb.permute(0, 2, 3, 1)\n        cr = cr.permute(0, 2, 3, 1)\n        return image[:, :, :, 0], cb.squeeze(3), cr.squeeze(3)\n\n\nclass block_splitting(nn.Module):\n    \"\"\" Splitting image into patches\n    Input:\n        image(tensor): batch x height x width\n    Output:\n        patch(tensor):  batch x h*w/64 x h x w\n    \"\"\"\n    def __init__(self):\n        super(block_splitting, self).__init__()\n        self.k = 8\n\n    def forward(self, image):\n        height, width = image.shape[1:3]\n        batch_size = image.shape[0]\n        image_reshaped = image.view(batch_size, height // self.k, self.k, -1, self.k)\n        image_transposed = image_reshaped.permute(0, 1, 3, 2, 4)\n        return image_transposed.contiguous().view(batch_size, -1, self.k, self.k)\n\n\nclass dct_8x8(nn.Module):\n    \"\"\" Discrete Cosine Transformation\n    Input:\n        image(tensor): batch x height x width\n    Output:\n        dcp(tensor): batch x height x width\n    \"\"\"\n    def __init__(self):\n        super(dct_8x8, self).__init__()\n        tensor = np.zeros((8, 8, 8, 8), dtype=np.float32)\n        for x, y, u, v in itertools.product(range(8), repeat=4):\n            tensor[x, y, u, v] = np.cos((2 * x + 1) * u * np.pi / 16) * np.cos(\n                (2 * y + 1) * v * np.pi / 16)\n        alpha = np.array([1. / np.sqrt(2)] + [1] * 7)\n        #\n        self.tensor =  nn.Parameter(torch.from_numpy(tensor).float())\n        self.scale = nn.Parameter(torch.from_numpy(np.outer(alpha, alpha) * 0.25).float() )\n\n    def forward(self, image):\n        image = image - 128\n        result = self.scale * torch.tensordot(image, self.tensor, dims=2)\n        result.view(image.shape)\n        return result\n\n\nclass y_quantize(nn.Module):\n    \"\"\" JPEG Quantization for Y channel\n    Input:\n        image(tensor): batch x height x width\n        rounding(function): rounding function to use\n        factor(float): Degree of compression\n    Output:\n        image(tensor): batch x height x width\n    \"\"\"\n    def __init__(self, rounding):\n        super(y_quantize, self).__init__()\n        self.rounding = rounding\n        self.y_table = y_table\n\n    def forward(self, image, factor):\n        image = image.float() / (self.y_table * factor)\n        image = self.rounding(image)\n        return image\n\n\nclass c_quantize(nn.Module):\n    \"\"\" JPEG Quantization for CrCb channels\n    Input:\n        image(tensor): batch x height x width\n        rounding(function): rounding function to use\n        factor(float): Degree of compression\n    Output:\n        image(tensor): batch x height x width\n    \"\"\"\n    def __init__(self, rounding):\n        super(c_quantize, self).__init__()\n        self.rounding = rounding\n        self.c_table = c_table\n\n    def forward(self, image, factor):\n        image = image.float() / (self.c_table * factor)\n        image = self.rounding(image)\n        return image\n\n\nclass compress_jpeg(nn.Module):\n    \"\"\" Full JPEG compression algortihm\n    Input:\n        imgs(tensor): batch x 3 x height x width\n        rounding(function): rounding function to use\n        factor(float): Compression factor\n    Ouput:\n        compressed(dict(tensor)): batch x h*w/64 x 8 x 8\n    \"\"\"\n    def __init__(self, rounding=torch.round):\n        super(compress_jpeg, self).__init__()\n        self.l1 = nn.Sequential(\n            rgb_to_ycbcr_jpeg(),\n            chroma_subsampling()\n        )\n        self.l2 = nn.Sequential(\n            block_splitting(),\n            dct_8x8()\n        )\n        self.c_quantize = c_quantize(rounding=rounding)\n        self.y_quantize = y_quantize(rounding=rounding)\n\n    def forward(self, image, factor=1):\n        y, cb, cr = self.l1(image*255)\n        components = {'y': y, 'cb': cb, 'cr': cr}\n        for k in components.keys():\n            comp = self.l2(components[k])\n            if k in ('cb', 'cr'):\n                comp = self.c_quantize(comp, factor)\n            else:\n                comp = self.y_quantize(comp, factor)\n\n            components[k] = comp\n\n        return components['y'], components['cb'], components['cr']\n\nclass DiffJPEG(nn.Module):\n    def __init__(self, height, width, differentiable=True):\n        ''' Initialize the DiffJPEG layer\n        Inputs:\n            height(int): Original image hieght\n            width(int): Original image width\n            differentiable(bool): If true uses custom differentiable\n                rounding function, if false uses standrard torch.round\n            quality(float): Quality factor for jpeg compression scheme.\n        '''\n        super(DiffJPEG, self).__init__()\n        if differentiable:\n            rounding = diff_round\n        else:\n            rounding = torch.round\n        self.compress = compress_jpeg(rounding=rounding)\n        self.decompress = decompress_jpeg(height, width, rounding=rounding)\n\n    def forward(self, x, quality=80):\n        '''\n\n        '''\n\n        factor = quality_to_factor(quality)\n        if factor == 0:\n          return x\n\n        y, cb, cr = self.compress(x, factor)\n        recovered = self.decompress(y, cb, cr, factor)\n        return recovered","metadata":{"execution":{"iopub.status.busy":"2024-04-27T11:27:37.576841Z","iopub.execute_input":"2024-04-27T11:27:37.577238Z","iopub.status.idle":"2024-04-27T11:27:37.773611Z","shell.execute_reply.started":"2024-04-27T11:27:37.577185Z","shell.execute_reply":"2024-04-27T11:27:37.772566Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SEED = hyperparams['SEED']\nstrength = hyperparams['STRENGTH']\nguidance_scale = hyperparams['GUIDANCE_SCALE']\nnum_inference_steps = hyperparams['NUM_DENOISING_STEPS']\nnum_images = len(image_url)\nimage_tensor = torch.zeros((num_images,3,512,512)).half()\nloss = []\n\n#Save individual image tensors\nsave_image_tensor = f'/kaggle/working/ImageTensor'\nif not os.path.exists(save_image_tensor):\n    os.mkdir(save_image_tensor)\n\nheight, width = (512,512)  #HardCoded\njpeg = DiffJPEG(height=height, width=width, differentiable=True).to('cuda')  #Change\n\nfor i in range(num_images):\n    init_seeds(SEED)\n    prompt_guidance = hyperparams['PROMPT_ATTACK']\n    init_image = download_image(image_url[i])\n    if init_image == None:\n        continue\n    X = preprocess(init_image).half().to(\"cuda\")\n    start = time.time()\n    result, last_imag, loss_sample = super_linf(X,\n                      jpeg_model = jpeg,\n                      prompt=prompt_guidance,\n                      eps=hyperparams['EPS'],\n                      step_size=hyperparams['STEP_SIZE'],\n                      iters=hyperparams['ITERS'],\n                      clamp_min = hyperparams['CLAMP_MIN'],\n                      clamp_max = hyperparams['CLAMP_MAX'],\n                      num_inference_steps=num_inference_steps,\n                      guidance_scale=guidance_scale,\n                      grad_reps=hyperparams['GRAD_REPS']\n                     )\n    torch.save(result.cpu(), f\"{save_image_tensor}/{i+offset}_result_image.pt\")\n    total_time = time.time() - start\n    image_tensor[i, :, :, :] = result.cpu()\n    loss.append(loss_sample)\n    del result, last_imag, X\n    torch.cuda.empty_cache()\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-04-27T11:27:37.775020Z","iopub.execute_input":"2024-04-27T11:27:37.775694Z","iopub.status.idle":"2024-04-27T11:34:19.631645Z","shell.execute_reply.started":"2024-04-27T11:27:37.775665Z","shell.execute_reply":"2024-04-27T11:34:19.629901Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"AVG Loss: 460.938: 100%|| 1/1 [00:14<00:00, 14.30s/it]\nAVG Loss: 361.688: 100%|| 1/1 [00:12<00:00, 12.41s/it]\nAVG Loss: 553.188: 100%|| 1/1 [00:12<00:00, 12.48s/it]\n","output_type":"stream"},{"name":"stdout","text":"Error downloading/processing image: cannot identify image file <_io.BytesIO object at 0x7e0b376291c0>\n","output_type":"stream"},{"name":"stderr","text":"AVG Loss: 453.562: 100%|| 1/1 [00:12<00:00, 12.52s/it]\nAVG Loss: 543.000: 100%|| 1/1 [00:12<00:00, 12.64s/it]\nAVG Loss: 542.688: 100%|| 1/1 [00:12<00:00, 12.65s/it]\nAVG Loss: 554.500: 100%|| 1/1 [00:12<00:00, 12.69s/it]\nAVG Loss: 485.938: 100%|| 1/1 [00:12<00:00, 12.68s/it]\nAVG Loss: 569.000: 100%|| 1/1 [00:12<00:00, 12.71s/it]\nAVG Loss: 464.688: 100%|| 1/1 [00:12<00:00, 12.77s/it]\nAVG Loss: 464.125: 100%|| 1/1 [00:12<00:00, 12.74s/it]\nAVG Loss: 370.562: 100%|| 1/1 [00:12<00:00, 12.81s/it]\nAVG Loss: 541.375: 100%|| 1/1 [00:12<00:00, 12.83s/it]\nAVG Loss: 577.875: 100%|| 1/1 [00:12<00:00, 12.90s/it]\nAVG Loss: 533.875: 100%|| 1/1 [00:12<00:00, 12.85s/it]\nAVG Loss: 473.688: 100%|| 1/1 [00:12<00:00, 12.84s/it]\nAVG Loss: 401.562: 100%|| 1/1 [00:12<00:00, 12.92s/it]\n","output_type":"stream"},{"name":"stdout","text":"Error downloading/processing image: cannot identify image file <_io.BytesIO object at 0x7e0c8183d490>\n","output_type":"stream"},{"name":"stderr","text":"AVG Loss: 485.062: 100%|| 1/1 [00:12<00:00, 12.91s/it]\nAVG Loss: 459.125: 100%|| 1/1 [00:12<00:00, 12.90s/it]\nAVG Loss: 427.188: 100%|| 1/1 [00:12<00:00, 12.92s/it]\nAVG Loss: 435.938: 100%|| 1/1 [00:12<00:00, 12.96s/it]\nAVG Loss: 491.625: 100%|| 1/1 [00:12<00:00, 12.96s/it]\nAVG Loss: 528.750: 100%|| 1/1 [00:12<00:00, 12.95s/it]\nAVG Loss: 409.062: 100%|| 1/1 [00:12<00:00, 12.91s/it]\nAVG Loss: 480.938: 100%|| 1/1 [00:12<00:00, 12.96s/it]\nAVG Loss: 536.500: 100%|| 1/1 [00:12<00:00, 12.96s/it]\nAVG Loss: 441.312: 100%|| 1/1 [00:12<00:00, 12.93s/it]\nAVG Loss: 410.812: 100%|| 1/1 [00:12<00:00, 12.95s/it]\nAVG Loss: 460.375: 100%|| 1/1 [00:12<00:00, 12.96s/it]\n  0%|          | 0/1 [00:02<?, ?it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[24], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m X \u001b[38;5;241m=\u001b[39m preprocess(init_image)\u001b[38;5;241m.\u001b[39mhalf()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 25\u001b[0m result, last_imag, loss_sample \u001b[38;5;241m=\u001b[39m \u001b[43msuper_linf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mjpeg_model\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mjpeg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_guidance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m                  \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mEPS\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mstep_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSTEP_SIZE\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m                  \u001b[49m\u001b[43miters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mITERS\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mclamp_min\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mhyperparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCLAMP_MIN\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mclamp_max\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mhyperparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCLAMP_MAX\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mnum_inference_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_inference_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mguidance_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mgrad_reps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mGRAD_REPS\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m                 \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(result\u001b[38;5;241m.\u001b[39mcpu(), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_image_tensor\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39moffset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_result_image.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     38\u001b[0m total_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start\n","Cell \u001b[0;32mIn[21], line 138\u001b[0m, in \u001b[0;36msuper_linf\u001b[0;34m(X, jpeg_model, prompt, step_size, iters, eps, clamp_min, clamp_max, grad_reps, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m all_grads \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(grad_reps):\n\u001b[0;32m--> 138\u001b[0m     c_grad, loss, last_image \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_jpeg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m     all_grads\u001b[38;5;241m.\u001b[39mappend(c_grad)\n\u001b[1;32m    140\u001b[0m     losses\u001b[38;5;241m.\u001b[39mappend(loss)\n","Cell \u001b[0;32mIn[21], line 84\u001b[0m, in \u001b[0;36mcompute_grad\u001b[0;34m(X, cur_image, prompt, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m#     loss = (image_nat - target_image).norm(p=2)\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m#     loss = lpips(image.clamp(-1,1), X)\u001b[39;00m\n\u001b[1;32m     83\u001b[0m     loss \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mnorm(p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 84\u001b[0m     grad \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mcur_image\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m grad, loss\u001b[38;5;241m.\u001b[39mitem(), image\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mcpu()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:303\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(grad_outputs_)\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    304\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"to_pil = T.ToPILImage()\nmodel_id_or_path = \"runwayml/stable-diffusion-v1-5\"\n# model_id_or_path = \"CompVis/stable-diffusion-v1-4\"\n# model_id_or_path = \"CompVis/stable-diffusion-v1-3\"\n# model_id_or_path = \"CompVis/stable-diffusion-v1-2\"\n# model_id_or_path = \"CompVis/stable-diffusion-v1-1\"\nos.environ['HF_HUB_OFFLINE']='True'\n\npipe_img2img = StableDiffusionImg2ImgPipeline.from_pretrained(\n    model_id_or_path,\n    revision=\"fp16\",\n    safety_checker=None,\n    torch_dtype=torch.float16,\n    cache_dir = \"/content/drive/MyDrive/MajorProject/Diffusers_cache_test/\"\n)\npipe_img2img = pipe_img2img.to('cuda')","metadata":{"execution":{"iopub.status.busy":"2024-04-27T11:34:19.632960Z","iopub.status.idle":"2024-04-27T11:34:19.633504Z","shell.execute_reply.started":"2024-04-27T11:34:19.633245Z","shell.execute_reply":"2024-04-27T11:34:19.633272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\nfor i in range(num_images):\n    result = image_tensor[i].unsqueeze(0).cuda()\n    adv_X = (result / 2 + 0.5).clamp(0, 1)    \n    adv_image = to_pil(adv_X[0]).convert(\"RGB\")\n    num_inference_steps = hyperparams['NUM_DENOISING_STEPS_FINAL']\n    prompt_image = prompt[i]\n    torch.cuda.empty_cache()\n    \n    path_to_save = f'/kaggle/working/{i+offset}/'\n    if not os.path.exists(path_to_save):\n        os.mkdir(path_to_save)\n    \n    init_image = download_image(image_url[i])\n    if init_image == None:\n        continue\n    gen_image(init_image, adv_image, path_to_save, prompt_image, QF)\n    save_dict_to_text_file( hyperparams, path_to_save, total_time, loss)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-27T11:34:19.635432Z","iopub.status.idle":"2024-04-27T11:34:19.635882Z","shell.execute_reply.started":"2024-04-27T11:34:19.635661Z","shell.execute_reply":"2024-04-27T11:34:19.635682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip -r fileensemblediffusion10.zip /kaggle/working\nfrom IPython.display import FileLink\nFileLink(r'fileensemblediffusion10.zip')","metadata":{"execution":{"iopub.status.busy":"2024-04-27T11:34:19.637355Z","iopub.status.idle":"2024-04-27T11:34:19.637841Z","shell.execute_reply.started":"2024-04-27T11:34:19.637582Z","shell.execute_reply":"2024-04-27T11:34:19.637607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}